{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping de archivos TLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este archivo se elabora un script para descargar los archivos desde la web de TLC de forma automática.\n",
    "- El mismo se fija si existe un archivo nuevo que no se encuentre descargado todavía y lo descarga, conseiderando el período Enero 2021 a la actualidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL_Viajes_Diarios (output_folder,year_month):\n",
    "        \n",
    "    # Yellow Taxi Tripdata\n",
    "    df_YT = pd.read_parquet(output_folder + \"\\\\yellow_tripdata_\" + year_month + \".parquet\")\n",
    "    # Green Taxi Tripdata\n",
    "    df_GT = pd.read_parquet(output_folder + \"\\\\green_tripdata_\" + year_month + \".parquet\")\n",
    "    # FHV - High Volume\n",
    "    df_FHVHV = pd.read_parquet(output_folder + \"\\\\fhvhv_tripdata_\" + year_month + \".parquet\")\n",
    "    # FHV - Other\n",
    "    df_FHV = pd.read_parquet(output_folder + \"\\\\fhv_tripdata_\" + year_month + \".parquet\")\n",
    "\n",
    "    # Eliminar datos duplicados\n",
    "    df_YT = df_YT.drop_duplicates()\n",
    "    df_GT = df_GT.drop_duplicates()\n",
    "    df_FHVHV = df_FHVHV.drop_duplicates()\n",
    "    df_FHV = df_FHV.drop_duplicates()\n",
    "\n",
    "    # Se renombran las columnas de cada DF\n",
    "    df_YT = df_YT.rename(columns={\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id'\n",
    "    })\n",
    "\n",
    "    df_GT = df_GT.rename(columns={\n",
    "        'lpep_pickup_datetime': 'pickup_datetime',\n",
    "        'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id'\n",
    "    })\n",
    "\n",
    "    df_FHVHV = df_FHVHV.rename(columns={\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id',\n",
    "        'trip_miles': 'trip_distance',\n",
    "        'base_passenger_fare': 'fare_amount'\n",
    "    })\n",
    "\n",
    "    df_FHV = df_FHV.rename(columns={\n",
    "        'dropOff_datetime': 'dropoff_datetime',\n",
    "        'PUlocationID': 'pickup_location_id',\n",
    "        'DOlocationID': 'dropoff_location_id',\n",
    "        'SR_Flag': 'shared_match_flag'\n",
    "    })\n",
    "\n",
    "    # Reemplazo \"Y\" por True y otros valores por False\n",
    "    df_FHVHV['shared_request_flag'] = df_FHVHV['shared_request_flag'].map({'Y': True}).fillna(False)\n",
    "    df_FHVHV['shared_match_flag'] = df_FHVHV['shared_match_flag'].map({'Y': True}).fillna(False)\n",
    "    df_FHV['shared_match_flag'] = df_FHV['shared_match_flag'].map({1: True}).fillna(False)\n",
    "\n",
    "    # Se crean las columnas 'shared_request_flag' y 'shared_match_flag' con valores nulos\n",
    "    df_YT['shared_request_flag'] = np.nan\n",
    "    df_YT['shared_match_flag'] = np.nan\n",
    "    df_GT['shared_request_flag'] = np.nan\n",
    "    df_GT['shared_match_flag'] = np.nan\n",
    "\n",
    "    # Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "    df_FHVHV['passenger_count'] = np.nan\n",
    "    df_FHVHV['payment_type'] = np.nan\n",
    "\n",
    "    # Se crea la columna 'total_amount' sumando los valores de las columnas indicadas en el dataset df_FHVHV\n",
    "    df_FHVHV['total_amount'] = (\n",
    "        df_FHVHV['fare_amount'] +\n",
    "        df_FHVHV['sales_tax'] +\n",
    "        df_FHVHV['bcf'] +\n",
    "        df_FHVHV['tips'] +\n",
    "        df_FHVHV['tolls']\n",
    "    )\n",
    "\n",
    "    # Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "    df_FHV['passenger_count'] = np.nan\n",
    "    df_FHV['trip_distance'] = np.nan\n",
    "    df_FHV['payment_type'] = np.nan\n",
    "    df_FHV['fare_amount'] = np.nan\n",
    "    df_FHV['total_amount'] = np.nan\n",
    "    df_FHV['shared_request_flag'] = np.nan\n",
    "    df_FHV['congestion_surcharge'] = np.nan\n",
    "\n",
    "    # Clasificación\n",
    "    df_YT['industry'] = 'Yellow Taxi'\n",
    "    df_GT['industry'] = 'Green Taxi'\n",
    "    df_FHVHV['industry'] = 'FHV - High Volume'\n",
    "    df_FHV['industry'] = 'FHV - Other'\n",
    "\n",
    "\n",
    "    # Lista de columnas que se conservan de todos los df\n",
    "    columnas_a_conservar = [\n",
    "        'industry',\n",
    "        'pickup_datetime',\n",
    "        'dropoff_datetime',\n",
    "        'pickup_location_id',\n",
    "        'dropoff_location_id',\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'payment_type',\n",
    "        'fare_amount',\n",
    "        'total_amount',\n",
    "        'congestion_surcharge',\n",
    "        'shared_request_flag',\n",
    "        'shared_match_flag'\n",
    "    ]\n",
    "\n",
    "    # Elimino las columnas que no necesito.\n",
    "    df_YT = df_YT.loc[:, columnas_a_conservar]\n",
    "    df_GT = df_GT.loc[:, columnas_a_conservar]\n",
    "    df_FHVHV = df_FHVHV.loc[:, columnas_a_conservar]\n",
    "    df_FHV = df_FHV.loc[:, columnas_a_conservar]\n",
    "\n",
    "    # Lista de dataframes\n",
    "    dataframes = [df_YT, df_GT, df_FHVHV, df_FHV]\n",
    "    # Se concatenean los df con todas las columnas y se reinicia el índice.\n",
    "    df = pd.concat(dataframes, join='inner', ignore_index=True)\n",
    "\n",
    "    # Se eliminan los DataFrames originales\n",
    "    del df_YT, df_GT, df_FHVHV, df_FHV\n",
    "    # Se llama al recolector de basura para liberar la memoria\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Convierto los tipos de datos\n",
    "    df['pickup_datetime'] = df['pickup_datetime'].astype('datetime64[ns]')\n",
    "    df['dropoff_datetime'] = df['dropoff_datetime'].astype('datetime64[ns]')\n",
    "    df['industry'] = df['industry'].astype('string')\n",
    "    df['pickup_location_id'] = df['pickup_location_id'].astype('Int64')\n",
    "    df['dropoff_location_id'] = df['dropoff_location_id'].astype('Int64')\n",
    "    df['passenger_count'] = df['passenger_count'].astype('Int64')\n",
    "    df['payment_type'] = df['payment_type'].astype('Int64')\n",
    "    df['shared_request_flag'] = df['shared_request_flag'].astype('boolean')\n",
    "    df['shared_match_flag'] = df['shared_match_flag'].astype('boolean')\n",
    "\n",
    "\n",
    "    # Identificar fechas fuera de rango \n",
    "    # Este rango deberá ser variable en función del mes de los archivos levantados para el ETL de carga en la Nube)\n",
    "    # fecha_inicio = '2020-12-01'\n",
    "    # fecha_fin = '2021-02-28'\n",
    "    fecha_inicio = datetime.strptime(year_month + \"-01\", \"%Y-%m-%d\")\n",
    "    fecha_fin = fecha_inicio + MonthEnd(0)\n",
    "\n",
    "\n",
    "    df = df[(df['pickup_datetime'] >= fecha_inicio) & (df['pickup_datetime'] <= fecha_fin)]\n",
    "    df = df[(df['dropoff_datetime'] >= fecha_inicio) & (df['dropoff_datetime'] <= fecha_fin)]\n",
    "\n",
    "    #Se reemplazan valores 0 por nulos\n",
    "    df['passenger_count'] = df['passenger_count'].replace(0, np.nan)\n",
    "\n",
    "    # Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "    # Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "    Q1 = df['trip_distance'].quantile(0.25)\n",
    "    Q3 = df['trip_distance'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df['trip_distance'] = np.where( (df['trip_distance'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['trip_distance'] > Q3 + 1.5 * IQR), np.nan,\n",
    "        df['trip_distance'])\n",
    "\n",
    "    # Elimino los registros con nulos en pickup o dropoff\n",
    "    df_cleaned = df.dropna(subset=['pickup_location_id', 'dropoff_location_id'])\n",
    "\n",
    "    # Calcuo la distancia promedio por combinación de pickup_location_id y dropoff_location_id\n",
    "    distancia_promedio = df_cleaned.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_distance'].mean().reset_index()\n",
    "    distancia_promedio.columns = ['pickup_location_id', 'dropoff_location_id', 'promedio_distancia']\n",
    "\n",
    "    # Combino el DataFrame original con el DataFrame de distancia promedio\n",
    "    df = df_cleaned.merge(distancia_promedio, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "    # Relleno los valores nulos en trip_distance con la distancia promedio\n",
    "    df['trip_distance'] = df['trip_distance'].fillna(df['promedio_distancia'])\n",
    "\n",
    "    # Elimino la columna temporal de promedio de distancia\n",
    "    df.drop(columns=['promedio_distancia'], inplace=True)\n",
    "\n",
    "    # Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "    # Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "    Q1 = df['fare_amount'].quantile(0.25)\n",
    "    Q3 = df['fare_amount'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df['fare_amount'] = np.where( (df['fare_amount'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['fare_amount'] > Q3 + 1.5 * IQR), np.nan,\n",
    "        df['fare_amount'])\n",
    "\n",
    "    # No puede haber valores negativos\n",
    "    df['total_amount'] = np.where(df['total_amount'] < 0, np.nan, df['total_amount'])\n",
    "\n",
    "    # Pongo nulo en total_amount si en fare_amount hay nulo\n",
    "    df.loc[df['fare_amount'].isna(), 'total_amount'] = np.nan\n",
    "\n",
    "    # Elimino outliers a partir de los residuos de la correlación con fare_amount\n",
    "    # Elimino los nulos\n",
    "    df_no_nan = df.dropna(subset=['fare_amount', 'total_amount'])\n",
    "\n",
    "    # Defino las variables a correlacional\n",
    "    X = df_no_nan[['fare_amount']]\n",
    "    y = df_no_nan['total_amount']\n",
    "\n",
    "    # Ajusto el modelo de regresión lineal\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predigo los valores de 'total_amount'\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Calculo los residuos\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Defino un umbral en el percentil 99% para identificar outliers\n",
    "    threshold = np.percentile(np.abs(residuals), 99.99)\n",
    "\n",
    "    # Índices donde el residuo supera el threshold\n",
    "    outlier_indices = df_no_nan.index[np.abs(residuals) > threshold]\n",
    "\n",
    "    # Pongo nulo cuando se supera el umbral en los índices que superaron el threshold\n",
    "    df.loc[outlier_indices, 'total_amount'] = np.nan\n",
    "\n",
    "    # No puede haber valores negativos\n",
    "    df['congestion_surcharge'] = np.where(df['congestion_surcharge'] < 0, np.nan, df['congestion_surcharge'])\n",
    "\n",
    "    # Configurar el valor de shared_match_flag basado en passenger_count\n",
    "    df['shared_match_flag'] = df['passenger_count'].apply(lambda x: True if x > 1 else False)\n",
    "\n",
    "    # Máscara donde dropoff_datetime es menor que pickup_datetime\n",
    "    mask = df['dropoff_datetime'] < df['pickup_datetime']\n",
    "    # Intercambio los valores en esas filas\n",
    "    df.loc[mask, ['pickup_datetime', 'dropoff_datetime']] = df.loc[mask, ['dropoff_datetime', 'pickup_datetime']].values\n",
    "    # Recalculo la duración\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    # Se define el límite de valores de tiempo de viaje superiores a al percentil 99%\n",
    "    Limit = df['trip_duration'].quantile(.999)\n",
    "    duration_count_99 = df[df['trip_duration'] > Limit].groupby('industry').size().reset_index(name='count')\n",
    "\n",
    "    # Calculo la duración media de trip_duration por combinación de pickup y dropoff location\n",
    "    mean_duration_by_location = (\n",
    "        df.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'mean_trip_duration'})\n",
    "    )\n",
    "\n",
    "    # Uno el promedio de duración con el DataFrame original para usarlo en el reemplazo\n",
    "    df = df.merge(mean_duration_by_location, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "    # Reemplazo los valores de dropoff_datetime fuera del límite con pickup_datetime + mean_trip_duration\n",
    "    df.loc[df['trip_duration'] > Limit, 'dropoff_datetime'] = (\n",
    "        df['pickup_datetime'] + pd.to_timedelta(df['mean_trip_duration'], unit='s')\n",
    "    )\n",
    "\n",
    "    # Elimino la columna auxiliar 'mean_trip_duration'\n",
    "    df.drop(columns='mean_trip_duration', inplace=True)\n",
    "\n",
    "    # Recalculo la duración\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day_of_week'] = df['pickup_datetime'].dt.day_name()\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['day_of_week_num'] = df['pickup_datetime'].dt.dayofweek #(0=Lunes, ..., 6=Domingo)\n",
    "    df['date'] = pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "\n",
    "    # Cargamos el dataset de feriados y convertimos la columna 'date' a solo fecha\n",
    "    df_feriados = pd.read_csv(output_folder + \"\\\\feriados_nacionales_2021_2024.csv\")\n",
    "    df_feriados['date'] = pd.to_datetime(df_feriados['date']).dt.date\n",
    "\n",
    "    # Realizamos el merge para identificar los días feriados\n",
    "    df = df.merge(df_feriados[['date','name']], how='left', left_on='date', right_on='date')\n",
    "\n",
    "    # Creamos la columna 'is_holiday' que marca True si es feriado y False en caso contrario\n",
    "    df['is_holiday'] = df['name'].notna()\n",
    "\n",
    "    # Creamos la columna 'working_day'\n",
    "    # Marcamos como 'No Laborable' (False) los días feriados o los fines de semana (sábado y domingo)\n",
    "    df['working_day'] = ~((df['is_holiday']) | (df['day_of_week_num'] >= 5))\n",
    "\n",
    "    # Opcional: Convertimos el campo 'working_day' a 'Laborable' o 'No Laborable' para mejor comprensión\n",
    "    df['working_day'] = df['working_day'].map({True: 'Laborable', False: 'No Laborable'})\n",
    "\n",
    "    # Eliminamos columnas temporales si ya no son necesarias\n",
    "    df.drop(columns=['date', 'is_holiday'], inplace=True)\n",
    "\n",
    "\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # 1. DataFrame de agregaciones por mes, año y industry\n",
    "    df_by_industry = df.groupby(\n",
    "        ['year', 'month', 'industry']\n",
    "    ).agg(\n",
    "        total_trips=('pickup_datetime', 'count'),\n",
    "        passenger_count=('passenger_count', 'sum'),\n",
    "        trip_distance=('trip_distance', 'sum'),\n",
    "        trip_duration=('trip_duration', 'sum'),\n",
    "        avg_trip_distance=('trip_distance', 'mean'),\n",
    "        avg_trip_duration=('trip_duration', 'mean'),\n",
    "        fare_amount=('fare_amount', 'sum'),\n",
    "        total_amount=('total_amount', 'sum'),\n",
    "        shared_requests=('shared_match_flag', lambda x: (x == True).sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # 2. DataFrame de agregaciones por mes, año y combinación de pickup_location_id y dropoff_location_id\n",
    "    df_by_location = df.groupby(\n",
    "        ['year', 'month', 'pickup_location_id', 'dropoff_location_id']\n",
    "    ).agg(\n",
    "        total_trips=('pickup_datetime', 'count'),\n",
    "        passenger_count=('passenger_count', 'sum'),\n",
    "        trip_distance=('trip_distance', 'sum'),\n",
    "        trip_duration=('trip_duration', 'sum'),\n",
    "        avg_trip_distance=('trip_distance', 'mean'),\n",
    "        avg_trip_duration=('trip_duration', 'mean'), \n",
    "        fare_amount=('fare_amount', 'sum'),\n",
    "        total_amount=('total_amount', 'sum'),\n",
    "        shared_requests=('shared_match_flag', lambda x: (x == True).sum())\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # Guarda los datos consolidados en csv\n",
    "    file_industry = output_folder + \"viajes_depurado_by_industry.csv\"    \n",
    "    file_location =output_folder + \"viajes_depurado_by_location.csv\"\n",
    "\n",
    "    # Función para anexar o crear archivo CSV\n",
    "    def append_to_csv(file_path, new_data):\n",
    "        # Si el archivo existe, cargar y anexar datos; si no, crear uno nuevo\n",
    "        if os.path.exists(file_path):\n",
    "            existing_data = pd.read_csv(file_path)\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "        else:\n",
    "            combined_data = new_data\n",
    "        \n",
    "        # Guardar el DataFrame resultante en el archivo CSV\n",
    "        combined_data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Anexar los datos para cada archivo\n",
    "    append_to_csv(file_industry, df_by_industry)\n",
    "    append_to_csv(file_location, df_by_location)\n",
    "\n",
    "    # Guarda el DataFrame en un archivo parquet\n",
    "    df.to_parquet(output_folder + \"\\\\viajes_depurado_\" + year_month + \".parquet\", engine='pyarrow', index=False)\n",
    "\n",
    "    print (\"ETL Procesado OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Procesado OK\n",
      "ETL Procesado OK\n",
      "ETL Procesado OK\n",
      "ETL Procesado OK\n",
      "ETL Procesado OK\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m fechas_csv \u001b[38;5;241m=\u001b[39m output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFechas_Archivos_Levantados.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m start_date \u001b[38;5;241m=\u001b[39m get_start_date_from_csv(fechas_csv)\n\u001b[1;32m--> 112\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfechas_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 47\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(base_urls, output_folder, start_date, csv_url, fechas_csv, retries, delay)\u001b[0m\n\u001b[0;32m     44\u001b[0m download_single_file(csv_url, csv_file_path, retries, delay)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Realiza el ETL de los 4 archivos parquet\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mETL_Viajes_Diarios\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43myear_month\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Guardar la fecha en Fechas_Archivos_Levantados.csv\u001b[39;00m\n\u001b[0;32m     50\u001b[0m save_date_to_csv(current_date, fechas_csv)\n",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m, in \u001b[0;36mETL_Viajes_Diarios\u001b[1;34m(output_folder, year_month)\u001b[0m\n\u001b[0;32m      6\u001b[0m df_GT \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgreen_tripdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m year_month \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# FHV - High Volume\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_FHVHV \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mfhvhv_tripdata_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myear_month\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# FHV - Other\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_FHV \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfhv_tripdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m year_month \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hernán\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hernán\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Hernán\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hernán\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Función para leer la última fecha registrada y calcular el siguiente mes\n",
    "def get_start_date_from_csv(file_path):\n",
    "    try:\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            last_row = list(reader)[-1]  # Obtener la última fila\n",
    "            last_date = datetime.strptime(last_row[0], \"%d/%m/%Y\")  # Parsear la fecha en formato \"DD/MM/YYYY\"\n",
    "            return last_date + relativedelta(months=1)  # Retornar el mes siguiente\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error leyendo {file_path}: {e}\")\n",
    "        return datetime(2021, 1, 1)  # Valor predeterminado si no existe el archivo o falla la lectura\n",
    "\n",
    "\n",
    "# Función para verificar y descargar archivos .parquet\n",
    "def download_file(base_urls, output_folder, start_date, csv_url, fechas_csv, retries=3, delay=5):\n",
    "    \n",
    "    # Para cada fecha desde el inicio hasta el mes actual:\n",
    "    end_date = datetime.now()\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year_month = current_date.strftime(\"%Y-%m\")  # Formato año-mes\n",
    "        all_files_downloaded = True  # Bandera para verificar descarga de todos los archivos .parquet\n",
    "\n",
    "        # Se recorren todas las URL de los archivos parquet.\n",
    "        for dataset_type, base_url in base_urls.items():\n",
    "            \n",
    "            # Construir la URL y el nombre de archivo de destino\n",
    "            url = base_url.format(year_month)\n",
    "            file_name = f\"{dataset_type}_tripdata_{year_month}.parquet\"\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Intentar descargar el archivo si no existe\n",
    "            if not os.path.exists(file_path):\n",
    "                success = download_single_file(url, file_path, retries, delay)\n",
    "                if not success:\n",
    "                    all_files_downloaded = False  # Marcar como falso si falla alguna descarga\n",
    "\n",
    "        \n",
    "        # Si logró descargar los 4 archivos parquet del mes:\n",
    "        if all_files_downloaded:\n",
    "\n",
    "            # Descarga el archivo CSV mensual           \n",
    "            download_single_file(csv_url, csv_file_path, retries, delay)\n",
    "            \n",
    "            # Realiza el ETL de los 4 archivos parquet\n",
    "            ETL_Viajes_Diarios (output_folder,year_month)\n",
    "\n",
    "            # Guardar la fecha en Fechas_Archivos_Levantados.csv\n",
    "            save_date_to_csv(current_date, fechas_csv)\n",
    "\n",
    "        # Avanzar al siguiente mes\n",
    "        current_date += relativedelta(months=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para descargar un archivo individual con reintentos\n",
    "def download_single_file(url, file_path, retries=3, delay=5):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "            return True  # Éxito en la descarga\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                error_type = type(e).__name__\n",
    "                logging.error(f\"Error {error_type} en {file_path}: {e}\")\n",
    "            else:\n",
    "                time.sleep(delay)  # Esperar antes de intentar de nuevo\n",
    "    return False  # Falla después de todos los intentos\n",
    "\n",
    "\n",
    "\n",
    "# Función para guardar la fecha de descarga en el archivo CSV\n",
    "def save_date_to_csv(date, file_path):\n",
    "    try:\n",
    "        with open(file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([date.strftime(\"%d/%m/%Y\")])  # Guardar la fecha en formato \"DD/MM/YYYY\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error guardando fecha en {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################################\n",
    "################ MAIN ################\n",
    "################################################################################################################################################\n",
    "\n",
    "output_folder = r\"C:\\\\Users\\\\Hernán\\\\Documents\\\\datasets\\\\TLC Trip Record Data\"  # Ruta del Bucket\n",
    "\n",
    "\n",
    "# Configuración de logging para registrar errores en un archivo log\n",
    "log_file = output_folder + \"\\\\error_log.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuración de ruta y URLs base\n",
    "base_urls = {\n",
    "    \"yellow\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{}.parquet\",\n",
    "    \"green\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{}.parquet\",\n",
    "    \"fhv\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_{}.parquet\",\n",
    "    \"fhvhv\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_{}.parquet\",\n",
    "}\n",
    "\n",
    "csv_url = \"https://www.nyc.gov/assets/tlc/downloads/csv/data_reports_monthly.csv\"\n",
    "csv_file_path = output_folder + \"\\\\data_reports_monthly.csv\"\n",
    "\n",
    "\n",
    "# Llamada a la función principal\n",
    "fechas_csv = output_folder + \"\\\\Fechas_Archivos_Levantados.csv\"\n",
    "start_date = get_start_date_from_csv(fechas_csv)\n",
    "download_file(base_urls, output_folder, start_date, csv_url, fechas_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
