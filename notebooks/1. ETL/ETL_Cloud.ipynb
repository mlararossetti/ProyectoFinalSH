{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yellow Taxi Tripdata\n",
    "df_YT = pd.read_parquet(r'..\\..\\datasets\\1. Originales\\TLC Trip Record Data\\yellow_tripdata_2021-01.parquet')\n",
    "# Green Taxi Tripdata\n",
    "df_GT = pd.read_parquet(r'..\\..\\datasets\\1. Originales\\TLC Trip Record Data\\green_tripdata_2021-01.parquet')\n",
    "# FHV - High Volume\n",
    "df_FHVHV = pd.read_parquet(r'..\\..\\datasets\\1. Originales\\TLC Trip Record Data\\fhvhv_tripdata_2021-01.parquet')\n",
    "# FHV - Other\n",
    "df_FHV = pd.read_parquet(r'..\\..\\datasets\\1. Originales\\TLC Trip Record Data\\fhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar datos duplicados\n",
    "df_YT = df_YT.drop_duplicates()\n",
    "df_GT = df_GT.drop_duplicates()\n",
    "df_FHVHV = df_FHVHV.drop_duplicates()\n",
    "df_FHV = df_FHV.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombran las columnas de cada DF\n",
    "df_YT = df_YT.rename(columns={\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'PULocationID': 'pickup_location_id',\n",
    "    'DOLocationID': 'dropoff_location_id'\n",
    "})\n",
    "\n",
    "df_GT = df_GT.rename(columns={\n",
    "    'lpep_pickup_datetime': 'pickup_datetime',\n",
    "    'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'PULocationID': 'pickup_location_id',\n",
    "    'DOLocationID': 'dropoff_location_id'\n",
    "})\n",
    "\n",
    "df_FHVHV = df_FHVHV.rename(columns={\n",
    "    'PULocationID': 'pickup_location_id',\n",
    "    'DOLocationID': 'dropoff_location_id',\n",
    "    'trip_miles': 'trip_distance',\n",
    "    'base_passenger_fare': 'fare_amount'\n",
    "})\n",
    "\n",
    "df_FHV = df_FHV.rename(columns={\n",
    "    'dropOff_datetime': 'dropoff_datetime',\n",
    "    'PUlocationID': 'pickup_location_id',\n",
    "    'DOlocationID': 'dropoff_location_id',\n",
    "    'SR_Flag': 'shared_match_flag'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlussiatti\\AppData\\Local\\Temp\\ipykernel_19312\\3215700350.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_FHVHV['shared_request_flag'] = df_FHVHV['shared_request_flag'].map({'Y': True}).fillna(False)\n",
      "C:\\Users\\hlussiatti\\AppData\\Local\\Temp\\ipykernel_19312\\3215700350.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_FHVHV['shared_match_flag'] = df_FHVHV['shared_match_flag'].map({'Y': True}).fillna(False)\n",
      "C:\\Users\\hlussiatti\\AppData\\Local\\Temp\\ipykernel_19312\\3215700350.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_FHV['shared_match_flag'] = df_FHV['shared_match_flag'].map({1: True}).fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# Reemplazo \"Y\" por True y otros valores por False\n",
    "df_FHVHV['shared_request_flag'] = df_FHVHV['shared_request_flag'].map({'Y': True}).fillna(False)\n",
    "df_FHVHV['shared_match_flag'] = df_FHVHV['shared_match_flag'].map({'Y': True}).fillna(False)\n",
    "df_FHV['shared_match_flag'] = df_FHV['shared_match_flag'].map({1: True}).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean las columnas 'shared_request_flag' y 'shared_match_flag' con valores nulos\n",
    "df_YT['shared_request_flag'] = np.nan\n",
    "df_YT['shared_match_flag'] = np.nan\n",
    "df_GT['shared_request_flag'] = np.nan\n",
    "df_GT['shared_match_flag'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "df_FHVHV['passenger_count'] = np.nan\n",
    "df_FHVHV['payment_type'] = np.nan\n",
    "\n",
    "# Se crea la columna 'total_amount' sumando los valores de las columnas indicadas en el dataset df_FHVHV\n",
    "df_FHVHV['total_amount'] = (\n",
    "    df_FHVHV['fare_amount'] +\n",
    "    df_FHVHV['sales_tax'] +\n",
    "    df_FHVHV['bcf'] +\n",
    "    df_FHVHV['tips'] +\n",
    "    df_FHVHV['tolls']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "df_FHV['passenger_count'] = np.nan\n",
    "df_FHV['trip_distance'] = np.nan\n",
    "df_FHV['payment_type'] = np.nan\n",
    "df_FHV['fare_amount'] = np.nan\n",
    "df_FHV['total_amount'] = np.nan\n",
    "df_FHV['shared_request_flag'] = np.nan\n",
    "df_FHV['congestion_surcharge'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_YT['industry'] = 'Yellow Taxi'\n",
    "df_GT['industry'] = 'Green Taxi'\n",
    "df_FHVHV['industry'] = 'FHV - High Volume'\n",
    "df_FHV['industry'] = 'FHV - Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas que se conservan de todos los df\n",
    "columnas_a_conservar = [\n",
    "    'industry',\n",
    "    'pickup_datetime',\n",
    "    'dropoff_datetime',\n",
    "    'pickup_location_id',\n",
    "    'dropoff_location_id',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'payment_type',\n",
    "    'fare_amount',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'shared_request_flag',\n",
    "    'shared_match_flag'\n",
    "]\n",
    "\n",
    "# Elimino las columnas que no necesito.\n",
    "df_YT = df_YT.loc[:, columnas_a_conservar]\n",
    "df_GT = df_GT.loc[:, columnas_a_conservar]\n",
    "df_FHVHV = df_FHVHV.loc[:, columnas_a_conservar]\n",
    "df_FHV = df_FHV.loc[:, columnas_a_conservar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de dataframes\n",
    "dataframes = [df_YT, df_GT, df_FHVHV, df_FHV]\n",
    "# Se concatenean los df con todas las columnas y se reinicia el índice.\n",
    "df = pd.concat(dataframes, join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6617"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se eliminan los DataFrames originales\n",
    "del df_YT, df_GT, df_FHVHV, df_FHV\n",
    "# Se llama al recolector de basura para liberar la memoria\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierto los tipos de datos\n",
    "df['pickup_datetime'] = df['pickup_datetime'].astype('datetime64[ns]')\n",
    "df['dropoff_datetime'] = df['dropoff_datetime'].astype('datetime64[ns]')\n",
    "df['industry'] = df['industry'].astype('string')\n",
    "df['pickup_location_id'] = df['pickup_location_id'].astype('Int64')\n",
    "df['dropoff_location_id'] = df['dropoff_location_id'].astype('Int64')\n",
    "df['passenger_count'] = df['passenger_count'].astype('Int64')\n",
    "df['payment_type'] = df['payment_type'].astype('Int64')\n",
    "df['shared_request_flag'] = df['shared_request_flag'].astype('boolean')\n",
    "df['shared_match_flag'] = df['shared_match_flag'].astype('boolean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar fechas fuera de rango \n",
    "# Este rango deberá ser variable en función del mes de los archivos levantados para el ETL de carga en la Nube)\n",
    "fecha_inicio = '2020-12-01'\n",
    "fecha_fin = '2021-02-28'\n",
    "df = df[(df['pickup_datetime'] >= fecha_inicio) & (df['pickup_datetime'] <= fecha_fin)]\n",
    "df = df[(df['dropoff_datetime'] >= fecha_inicio) & (df['dropoff_datetime'] <= fecha_fin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se reemplazan valores 0 por nulos\n",
    "df['passenger_count'] = df['passenger_count'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "# Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "Q1 = df['trip_distance'].quantile(0.25)\n",
    "Q3 = df['trip_distance'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['trip_distance'] = np.where( (df['trip_distance'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['trip_distance'] > Q3 + 1.5 * IQR), np.nan,\n",
    "    df['trip_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino los registros con nulos en pickup o dropoff\n",
    "df_cleaned = df.dropna(subset=['pickup_location_id', 'dropoff_location_id'])\n",
    "\n",
    "# Calcuo la distancia promedio por combinación de pickup_location_id y dropoff_location_id\n",
    "distancia_promedio = df_cleaned.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_distance'].mean().reset_index()\n",
    "distancia_promedio.columns = ['pickup_location_id', 'dropoff_location_id', 'promedio_distancia']\n",
    "\n",
    "# Combino el DataFrame original con el DataFrame de distancia promedio\n",
    "df = df_cleaned.merge(distancia_promedio, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "# Relleno los valores nulos en trip_distance con la distancia promedio\n",
    "df['trip_distance'] = df['trip_distance'].fillna(df['promedio_distancia'])\n",
    "\n",
    "# Elimino la columna temporal de promedio de distancia\n",
    "df.drop(columns=['promedio_distancia'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "# Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "Q1 = df['fare_amount'].quantile(0.25)\n",
    "Q3 = df['fare_amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['fare_amount'] = np.where( (df['fare_amount'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['fare_amount'] > Q3 + 1.5 * IQR), np.nan,\n",
    "    df['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No puede haber valores negativos\n",
    "df['total_amount'] = np.where(df['total_amount'] < 0, np.nan, df['total_amount'])\n",
    "\n",
    "# Pongo nulo en total_amount si en fare_amount hay nulo\n",
    "df.loc[df['fare_amount'].isna(), 'total_amount'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino outliers a partir de los residuos de la correlación con fare_amount\n",
    "# Elimino los nulos\n",
    "df_no_nan = df.dropna(subset=['fare_amount', 'total_amount'])\n",
    "\n",
    "# Defino las variables a correlacional\n",
    "X = df_no_nan[['fare_amount']]\n",
    "y = df_no_nan['total_amount']\n",
    "\n",
    "# Ajusto el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predigo los valores de 'total_amount'\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculo los residuos\n",
    "residuals = y - predictions\n",
    "\n",
    "# Defino un umbral en el percentil 99% para identificar outliers\n",
    "threshold = np.percentile(np.abs(residuals), 99.99)\n",
    "\n",
    "# Índices donde el residuo supera el threshold\n",
    "outlier_indices = df_no_nan.index[np.abs(residuals) > threshold]\n",
    "\n",
    "# Pongo nulo cuando se supera el umbral en los índices que superaron el threshold\n",
    "df.loc[outlier_indices, 'total_amount'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No puede haber valores negativos\n",
    "df['congestion_surcharge'] = np.where(df['congestion_surcharge'] < 0, np.nan, df['congestion_surcharge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el valor de shared_match_flag basado en passenger_count\n",
    "df['shared_match_flag'] = df['passenger_count'].apply(lambda x: True if x > 1 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Máscara donde dropoff_datetime es menor que pickup_datetime\n",
    "mask = df['dropoff_datetime'] < df['pickup_datetime']\n",
    "# Intercambio los valores en esas filas\n",
    "df.loc[mask, ['pickup_datetime', 'dropoff_datetime']] = df.loc[mask, ['dropoff_datetime', 'pickup_datetime']].values\n",
    "# Recalculo la duración\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            industry  count\n",
      "0  FHV - High Volume   7795\n",
      "1        FHV - Other   2403\n",
      "2         Green Taxi    351\n",
      "3        Yellow Taxi   2988\n"
     ]
    }
   ],
   "source": [
    "# Se define el límite de valores de tiempo de viaje superiores a al percentil 99%\n",
    "Limit = df['trip_duration'].quantile(.999)\n",
    "duration_count_99 = df[df['trip_duration'] > Limit].groupby('industry').size().reset_index(name='count')\n",
    "\n",
    "# Calculo la duración media de trip_duration por combinación de pickup y dropoff location\n",
    "mean_duration_by_location = (\n",
    "    df.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_duration']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'trip_duration': 'mean_trip_duration'})\n",
    ")\n",
    "\n",
    "# Uno el promedio de duración con el DataFrame original para usarlo en el reemplazo\n",
    "df = df.merge(mean_duration_by_location, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "# Reemplazo los valores de dropoff_datetime fuera del límite con pickup_datetime + mean_trip_duration\n",
    "df.loc[df['trip_duration'] > Limit, 'dropoff_datetime'] = (\n",
    "    df['pickup_datetime'] + pd.to_timedelta(df['mean_trip_duration'], unit='s')\n",
    ")\n",
    "\n",
    "# Elimino la columna auxiliar 'mean_trip_duration'\n",
    "df.drop(columns='mean_trip_duration', inplace=True)\n",
    "\n",
    "# Recalculo la duración\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['pickup_datetime'].dt.year\n",
    "df['month'] = df['pickup_datetime'].dt.month\n",
    "df['day_of_week'] = df['pickup_datetime'].dt.day_name()\n",
    "df['hour'] = df['pickup_datetime'].dt.hour\n",
    "df['day_of_week_num'] = df['pickup_datetime'].dt.dayofweek #(0=Lunes, ..., 6=Domingo)\n",
    "df['date'] = pd.to_datetime(df['pickup_datetime']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de feriados y convertimos la columna 'date' a solo fecha\n",
    "df_feriados = pd.read_csv(r'..\\..\\datasets\\1. Originales\\feriados_nacionales_2021_2024.csv')\n",
    "df_feriados['date'] = pd.to_datetime(df_feriados['date']).dt.date\n",
    "\n",
    "# Realizamos el merge para identificar los días feriados\n",
    "df = df.merge(df_feriados[['date','name']], how='left', left_on='date', right_on='date')\n",
    "\n",
    "# Creamos la columna 'is_holiday' que marca True si es feriado y False en caso contrario\n",
    "df['is_holiday'] = df['name'].notna()\n",
    "\n",
    "# Creamos la columna 'working_day'\n",
    "# Marcamos como 'No Laborable' (False) los días feriados o los fines de semana (sábado y domingo)\n",
    "df['working_day'] = ~((df['is_holiday']) | (df['day_of_week_num'] >= 5))\n",
    "\n",
    "# Opcional: Convertimos el campo 'working_day' a 'Laborable' o 'No Laborable' para mejor comprensión\n",
    "df['working_day'] = df['working_day'].map({True: 'Laborable', False: 'No Laborable'})\n",
    "\n",
    "# Eliminamos columnas temporales si ya no son necesarias\n",
    "df.drop(columns=['date', 'is_holiday'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Cálculos adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precio Base por milla\n",
    "df['price_per_mile'] = df['fare_amount'] / df['trip_distance']  \n",
    "# Precio Total por milla\n",
    "df['total_price_per_mile'] = df['total_amount'] / df['trip_distance'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar los códigos numéricos con las etiquetas de los medios de pago\n",
    "df['payment_type_descr'] = df['payment_type'].map({\n",
    "    1: 'Credit card', \n",
    "    2: 'Cash', \n",
    "    3: 'No charge', \n",
    "    4: 'Dispute', \n",
    "    5: 'Unknown', \n",
    "    6: 'Voided trip'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda el DataFrame en un archivo parquet\n",
    "df.to_parquet(r'..\\..\\datasets\\2. Depurados\\TLC Trip Record Data\\viajes_depurado_2021_01.parquet', engine='pyarrow', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
