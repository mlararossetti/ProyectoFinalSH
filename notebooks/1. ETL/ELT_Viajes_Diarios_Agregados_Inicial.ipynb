{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping de archivos TLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este archivo se elabora un script para descargar los archivos desde la web de TLC de forma automática.\n",
    "- El mismo se fija si existe un archivo nuevo que no se encuentre descargado todavía y lo descarga, conseiderando el período Enero 2021 a la actualidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL_Viajes_Diarios (output_folder,year_month):\n",
    "    print(year_month)   \n",
    "    # Yellow Taxi Tripdata\n",
    "    df_YT = pd.read_parquet(output_folder + \"\\\\yellow_tripdata_\" + year_month + \".parquet\")\n",
    "    # Green Taxi Tripdata\n",
    "    df_GT = pd.read_parquet(output_folder + \"\\\\green_tripdata_\" + year_month + \".parquet\")\n",
    "    # FHV - High Volume\n",
    "    df_FHVHV = pd.read_parquet(output_folder + \"\\\\fhvhv_tripdata_\" + year_month + \".parquet\")\n",
    "    # FHV - Other\n",
    "    df_FHV = pd.read_parquet(output_folder + \"\\\\fhv_tripdata_\" + year_month + \".parquet\")\n",
    "\n",
    "    # Eliminar datos duplicados\n",
    "    df_YT = df_YT.drop_duplicates()\n",
    "    df_GT = df_GT.drop_duplicates()\n",
    "    df_FHVHV = df_FHVHV.drop_duplicates()\n",
    "    df_FHV = df_FHV.drop_duplicates()\n",
    "\n",
    "    # Se renombran las columnas de cada DF\n",
    "    df_YT = df_YT.rename(columns={\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id'\n",
    "    })\n",
    "\n",
    "    df_GT = df_GT.rename(columns={\n",
    "        'lpep_pickup_datetime': 'pickup_datetime',\n",
    "        'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id'\n",
    "    })\n",
    "\n",
    "    df_FHVHV = df_FHVHV.rename(columns={\n",
    "        'PULocationID': 'pickup_location_id',\n",
    "        'DOLocationID': 'dropoff_location_id',\n",
    "        'trip_miles': 'trip_distance',\n",
    "        'base_passenger_fare': 'fare_amount'\n",
    "    })\n",
    "\n",
    "    df_FHV = df_FHV.rename(columns={\n",
    "        'dropOff_datetime': 'dropoff_datetime',\n",
    "        'PUlocationID': 'pickup_location_id',\n",
    "        'DOlocationID': 'dropoff_location_id',\n",
    "        'SR_Flag': 'shared_match_flag'\n",
    "    })\n",
    "\n",
    "    # Reemplazo \"Y\" por True y otros valores por False\n",
    "    df_FHVHV['shared_request_flag'] = df_FHVHV['shared_request_flag'].map({'Y': True}).fillna(False)\n",
    "    df_FHVHV['shared_match_flag'] = df_FHVHV['shared_match_flag'].map({'Y': True}).fillna(False)\n",
    "    df_FHV['shared_match_flag'] = df_FHV['shared_match_flag'].map({1: True}).fillna(False)\n",
    "\n",
    "    # Se crean las columnas 'shared_request_flag' y 'shared_match_flag' con valores nulos\n",
    "    df_YT['shared_request_flag'] = np.nan\n",
    "    df_YT['shared_match_flag'] = np.nan\n",
    "    df_GT['shared_request_flag'] = np.nan\n",
    "    df_GT['shared_match_flag'] = np.nan\n",
    "\n",
    "    # Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "    df_FHVHV['passenger_count'] = np.nan\n",
    "    df_FHVHV['payment_type'] = np.nan\n",
    "\n",
    "    # Se crea la columna 'total_amount' sumando los valores de las columnas indicadas en el dataset df_FHVHV\n",
    "    df_FHVHV['total_amount'] = (\n",
    "        df_FHVHV['fare_amount'] +\n",
    "        df_FHVHV['sales_tax'] +\n",
    "        df_FHVHV['bcf'] +\n",
    "        df_FHVHV['tips'] +\n",
    "        df_FHVHV['tolls']\n",
    "    )\n",
    "\n",
    "    # Se crean las columnas 'passenger_count' y 'payment_type' con valores nulos\n",
    "    df_FHV['passenger_count'] = np.nan\n",
    "    df_FHV['trip_distance'] = np.nan\n",
    "    df_FHV['payment_type'] = np.nan\n",
    "    df_FHV['fare_amount'] = np.nan\n",
    "    df_FHV['total_amount'] = np.nan\n",
    "    df_FHV['shared_request_flag'] = np.nan\n",
    "    df_FHV['congestion_surcharge'] = np.nan\n",
    "\n",
    "    # Clasificación\n",
    "    df_YT['industry'] = 'Yellow Taxi'\n",
    "    df_GT['industry'] = 'Green Taxi'\n",
    "    df_FHVHV['industry'] = 'FHV - High Volume'\n",
    "    df_FHV['industry'] = 'FHV - Other'\n",
    "\n",
    "\n",
    "    # Lista de columnas que se conservan de todos los df\n",
    "    columnas_a_conservar = [\n",
    "        'industry',\n",
    "        'pickup_datetime',\n",
    "        'dropoff_datetime',\n",
    "        'pickup_location_id',\n",
    "        'dropoff_location_id',\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'payment_type',\n",
    "        'fare_amount',\n",
    "        'total_amount',\n",
    "        #'congestion_surcharge',\n",
    "        #'shared_request_flag',\n",
    "        'shared_match_flag'\n",
    "    ]\n",
    "\n",
    "    # Elimino las columnas que no necesito.\n",
    "    df_YT = df_YT.loc[:, columnas_a_conservar]\n",
    "    df_GT = df_GT.loc[:, columnas_a_conservar]\n",
    "    df_FHVHV = df_FHVHV.loc[:, columnas_a_conservar]\n",
    "    df_FHV = df_FHV.loc[:, columnas_a_conservar]\n",
    "\n",
    "    # Lista de dataframes\n",
    "    dataframes = [df_YT, df_GT, df_FHVHV, df_FHV]\n",
    "    # Se concatenean los df con todas las columnas y se reinicia el índice.\n",
    "    df = pd.concat(dataframes, join='inner', ignore_index=True)\n",
    "\n",
    "    # Se eliminan los DataFrames originales\n",
    "    del df_YT, df_GT, df_FHVHV, df_FHV\n",
    "    # Se llama al recolector de basura para liberar la memoria\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Convierto los tipos de datos\n",
    "    df['pickup_datetime'] = df['pickup_datetime'].astype('datetime64[ns]')\n",
    "    df['dropoff_datetime'] = df['dropoff_datetime'].astype('datetime64[ns]')\n",
    "    df['industry'] = df['industry'].astype('string')\n",
    "    df['pickup_location_id'] = df['pickup_location_id'].astype('Int64')\n",
    "    df['dropoff_location_id'] = df['dropoff_location_id'].astype('Int64')\n",
    "    df['passenger_count'] = df['passenger_count'].astype('Int64')\n",
    "    df['payment_type'] = df['payment_type'].astype('Int64')\n",
    "    #df['shared_request_flag'] = df['shared_request_flag'].astype('boolean')\n",
    "    df['shared_match_flag'] = df['shared_match_flag'].astype('boolean')\n",
    "\n",
    "\n",
    "    # Identificar fechas fuera de rango \n",
    "    # Este rango deberá ser variable en función del mes de los archivos levantados para el ETL de carga en la Nube)\n",
    "    fecha_inicio = datetime.strptime(year_month + \"-01\", \"%Y-%m-%d\")\n",
    "    fecha_fin = fecha_inicio + MonthEnd(0)\n",
    "\n",
    "\n",
    "    df = df[(df['pickup_datetime'] >= fecha_inicio) & (df['pickup_datetime'] <= fecha_fin)]\n",
    "    df = df[(df['dropoff_datetime'] >= fecha_inicio) & (df['dropoff_datetime'] <= fecha_fin)]\n",
    "\n",
    "    #Se reemplazan valores 0 por nulos\n",
    "    df['passenger_count'] = df['passenger_count'].replace(0, np.nan)\n",
    "\n",
    "    # Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "    # Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "    Q1 = df['trip_distance'].quantile(0.25)\n",
    "    Q3 = df['trip_distance'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df['trip_distance'] = np.where( (df['trip_distance'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['trip_distance'] > Q3 + 1.5 * IQR), np.nan,\n",
    "        df['trip_distance'])\n",
    "\n",
    "    # Elimino los registros con nulos en pickup o dropoff\n",
    "    df_cleaned = df.dropna(subset=['pickup_location_id', 'dropoff_location_id'])\n",
    "\n",
    "    # Calcuo la distancia promedio por combinación de pickup_location_id y dropoff_location_id\n",
    "    distancia_promedio = df_cleaned.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_distance'].mean().reset_index()\n",
    "    distancia_promedio.columns = ['pickup_location_id', 'dropoff_location_id', 'promedio_distancia']\n",
    "\n",
    "    # Combino el DataFrame original con el DataFrame de distancia promedio\n",
    "    df = df_cleaned.merge(distancia_promedio, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "    # Relleno los valores nulos en trip_distance con la distancia promedio\n",
    "    df['trip_distance'] = df['trip_distance'].fillna(df['promedio_distancia'])\n",
    "\n",
    "    # Elimino la columna temporal de promedio de distancia\n",
    "    df.drop(columns=['promedio_distancia'], inplace=True)\n",
    "\n",
    "    # Eliminación de outliers por método de Rango Intercuartílico con mínimo en 0.01.\n",
    "    # Se reemplazan los valores por nulos pero no se eliminan del dataset ya que cuentan para la cantidad de viajes.\n",
    "    Q1 = df['fare_amount'].quantile(0.25)\n",
    "    Q3 = df['fare_amount'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df['fare_amount'] = np.where( (df['fare_amount'] < max(0.01,  Q1 - 1.5 * IQR)) | (df['fare_amount'] > Q3 + 1.5 * IQR), np.nan,\n",
    "        df['fare_amount'])\n",
    "\n",
    "    # No puede haber valores negativos\n",
    "    df['total_amount'] = np.where(df['total_amount'] < 0, np.nan, df['total_amount'])\n",
    "\n",
    "    # Pongo nulo en total_amount si en fare_amount hay nulo\n",
    "    df.loc[df['fare_amount'].isna(), 'total_amount'] = np.nan\n",
    "\n",
    "    # Elimino outliers a partir de los residuos de la correlación con fare_amount\n",
    "    # Elimino los nulos\n",
    "    df_no_nan = df.dropna(subset=['fare_amount', 'total_amount'])\n",
    "\n",
    "    # Defino las variables a correlacional\n",
    "    X = df_no_nan[['fare_amount']]\n",
    "    y = df_no_nan['total_amount']\n",
    "\n",
    "    # Ajusto el modelo de regresión lineal\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predigo los valores de 'total_amount'\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Calculo los residuos\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Defino un umbral en el percentil 99% para identificar outliers\n",
    "    threshold = np.percentile(np.abs(residuals), 99.99)\n",
    "\n",
    "    # Índices donde el residuo supera el threshold\n",
    "    outlier_indices = df_no_nan.index[np.abs(residuals) > threshold]\n",
    "\n",
    "    # Pongo nulo cuando se supera el umbral en los índices que superaron el threshold\n",
    "    df.loc[outlier_indices, 'total_amount'] = np.nan\n",
    "\n",
    "    # No puede haber valores negativos\n",
    "    #df['congestion_surcharge'] = np.where(df['congestion_surcharge'] < 0, np.nan, df['congestion_surcharge'])\n",
    "\n",
    "    # Configurar el valor de shared_match_flag basado en passenger_count\n",
    "    df['shared_match_flag'] = df['passenger_count'].apply(lambda x: True if x > 1 else False)\n",
    "\n",
    "    # Máscara donde dropoff_datetime es menor que pickup_datetime\n",
    "    mask = df['dropoff_datetime'] < df['pickup_datetime']\n",
    "    # Intercambio los valores en esas filas\n",
    "    df.loc[mask, ['pickup_datetime', 'dropoff_datetime']] = df.loc[mask, ['dropoff_datetime', 'pickup_datetime']].values\n",
    "    # Recalculo la duración\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    # Se define el límite de valores de tiempo de viaje superiores a al percentil 99%\n",
    "    Limit = df['trip_duration'].quantile(.999)\n",
    "\n",
    "    # Calculo la duración media de trip_duration por combinación de pickup y dropoff location\n",
    "    mean_duration_by_location = (\n",
    "        df.groupby(['pickup_location_id', 'dropoff_location_id'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'mean_trip_duration'})\n",
    "    )\n",
    "\n",
    "    # Uno el promedio de duración con el DataFrame original para usarlo en el reemplazo\n",
    "    df = df.merge(mean_duration_by_location, on=['pickup_location_id', 'dropoff_location_id'], how='left')\n",
    "\n",
    "    # Reemplazo los valores de dropoff_datetime fuera del límite con pickup_datetime + mean_trip_duration\n",
    "    df.loc[df['trip_duration'] > Limit, 'dropoff_datetime'] = (\n",
    "        df['pickup_datetime'] + pd.to_timedelta(df['mean_trip_duration'], unit='s')\n",
    "    )\n",
    "\n",
    "    # Elimino la columna auxiliar 'mean_trip_duration'\n",
    "    df.drop(columns='mean_trip_duration', inplace=True)\n",
    "\n",
    "    # Recalculo la duración\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    # df['day_of_week'] = df['pickup_datetime'].dt.day_name()\n",
    "    # df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    # df['day_of_week_num'] = df['pickup_datetime'].dt.dayofweek #(0=Lunes, ..., 6=Domingo)\n",
    "    # df['date'] = pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "\n",
    "\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    # Creamos el campo que vincula los df\n",
    "    df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str), format='%Y-%m')\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m', errors='coerce')\n",
    "\n",
    "    # 1. DataFrame de agregaciones por mes, año y industry\n",
    "    df_by_industry = df.groupby(\n",
    "        ['date', 'industry']\n",
    "    ).agg(\n",
    "        total_trips=('pickup_datetime', 'count'),\n",
    "        passenger_count=('passenger_count', 'sum'),\n",
    "        trip_distance=('trip_distance', 'sum'),\n",
    "        trip_duration=('trip_duration', 'sum'),\n",
    "        avg_trip_distance=('trip_distance', 'mean'),\n",
    "        avg_trip_duration=('trip_duration', 'mean'),\n",
    "        fare_amount=('fare_amount', 'sum'),\n",
    "        total_amount=('total_amount', 'sum'),\n",
    "        shared_match_flag=('shared_match_flag', lambda x: (x == True).sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    # 2. DataFrame de agregaciones por mes, año y combinación de pickup_location_id y dropoff_location_id\n",
    "    df_by_location = df.groupby(\n",
    "        ['date', 'pickup_location_id', 'dropoff_location_id']\n",
    "    ).agg(\n",
    "        total_trips=('pickup_datetime', 'count'),\n",
    "        passenger_count=('passenger_count', 'sum'),\n",
    "        trip_distance=('trip_distance', 'sum'),\n",
    "        trip_duration=('trip_duration', 'sum'),\n",
    "        avg_trip_distance=('trip_distance', 'mean'),\n",
    "        avg_trip_duration=('trip_duration', 'mean'), \n",
    "        fare_amount=('fare_amount', 'sum'),\n",
    "        total_amount=('total_amount', 'sum'),\n",
    "        shared_match_flag=('shared_match_flag', lambda x: (x == True).sum())\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # Redondear las columnas de tipo DECIMAL antes de cargarlas en SQL\n",
    "    df_by_location['trip_distance'] = df_by_location['trip_distance'].round(2)\n",
    "    df_by_location['trip_duration'] = df_by_location['trip_duration'].round(2)\n",
    "    df_by_location['avg_trip_distance'] = df_by_location['avg_trip_distance'].round(2)  # DECIMAL(5, 2)\n",
    "    df_by_location['avg_trip_duration'] = df_by_location['avg_trip_duration'].round(2)  # DECIMAL(5, 2)\n",
    "\n",
    "\n",
    "    # Las columnas como 'pickup_location_id', 'dropoff_location_id', 'total_trips', 'passenger_count', y 'shared_match_flag'\n",
    "    # son de tipo INT y no requieren redondeo:\n",
    "    df_by_location['pickup_location_id'] = df_by_location['pickup_location_id'].astype('Int64')\n",
    "    df_by_location['dropoff_location_id'] = df_by_location['dropoff_location_id'].astype('Int64')\n",
    "    df_by_location['total_trips'] = df_by_location['total_trips'].astype('Int64')\n",
    "    df_by_location['passenger_count'] = df_by_location['passenger_count'].astype('Int64')\n",
    "    df_by_location['shared_match_flag'] = df_by_location['shared_match_flag'].astype('Int64')\n",
    "    # df_by_location['fare_amount'] = df_by_location['fare_amount'].astype('Int64')\n",
    "    # df_by_location['total_amount'] = df_by_location['total_amount'].astype('Int64')\n",
    "\n",
    "\n",
    "    # Guarda los datos consolidados en csv\n",
    "    file_industry = output_folder + \"\\\\TLC Trip Record Data_viajes_by_industry.csv\"    \n",
    "    file_location =output_folder + \"\\\\TLC Trip Record Data_viajes_by_location.csv\"\n",
    "\n",
    "    # Función para anexar o crear archivo CSV\n",
    "    def append_to_csv(file_path, new_data):\n",
    "        # Si el archivo existe, cargar y anexar datos; si no, crear uno nuevo\n",
    "        if os.path.exists(file_path):\n",
    "            existing_data = pd.read_csv(file_path)\n",
    "            existing_data['date'] = pd.to_datetime(existing_data['date'], format='%Y-%m-%d')\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "        else:\n",
    "            combined_data = new_data\n",
    "        \n",
    "        # Guardar el DataFrame resultante en el archivo CSV\n",
    "        combined_data.to_csv(file_path, index=False, date_format='%Y-%m-%d')\n",
    "\n",
    "        #combined_data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Anexar los datos para cada archivo\n",
    "    append_to_csv(file_industry, df_by_industry)\n",
    "    append_to_csv(file_location, df_by_location)\n",
    "\n",
    "    # Guarda el DataFrame en un archivo parquet\n",
    "    #df.to_parquet(output_folder + \"\\\\viajes_depurado_\" + year_month + \".parquet\", engine='pyarrow', index=False)\n",
    "\n",
    "    print (\"ETL Procesado OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01\n",
      "ETL Procesado OK\n",
      "2021-02\n",
      "ETL Procesado OK\n",
      "2021-03\n",
      "ETL Procesado OK\n",
      "2021-04\n",
      "ETL Procesado OK\n",
      "2021-05\n",
      "ETL Procesado OK\n",
      "2021-06\n",
      "ETL Procesado OK\n",
      "2021-07\n",
      "ETL Procesado OK\n",
      "2021-08\n",
      "ETL Procesado OK\n",
      "2021-09\n",
      "ETL Procesado OK\n",
      "2021-10\n",
      "ETL Procesado OK\n",
      "2021-11\n",
      "ETL Procesado OK\n",
      "2021-12\n",
      "ETL Procesado OK\n",
      "2022-01\n",
      "ETL Procesado OK\n",
      "2022-02\n",
      "ETL Procesado OK\n",
      "2022-03\n",
      "ETL Procesado OK\n",
      "2022-04\n",
      "ETL Procesado OK\n",
      "2022-05\n",
      "ETL Procesado OK\n",
      "2022-06\n",
      "ETL Procesado OK\n",
      "2022-07\n",
      "ETL Procesado OK\n",
      "2022-08\n",
      "ETL Procesado OK\n",
      "2022-09\n",
      "ETL Procesado OK\n",
      "2022-10\n",
      "ETL Procesado OK\n",
      "2022-11\n",
      "ETL Procesado OK\n",
      "2022-12\n",
      "ETL Procesado OK\n",
      "2023-01\n",
      "ETL Procesado OK\n",
      "2023-02\n",
      "ETL Procesado OK\n",
      "2023-03\n",
      "ETL Procesado OK\n",
      "2023-04\n",
      "ETL Procesado OK\n",
      "2023-05\n",
      "ETL Procesado OK\n",
      "2023-06\n",
      "ETL Procesado OK\n",
      "2023-07\n",
      "ETL Procesado OK\n",
      "2023-08\n",
      "ETL Procesado OK\n",
      "2023-09\n",
      "ETL Procesado OK\n",
      "2023-10\n",
      "ETL Procesado OK\n",
      "2023-11\n",
      "ETL Procesado OK\n",
      "2023-12\n",
      "ETL Procesado OK\n",
      "2024-01\n",
      "ETL Procesado OK\n",
      "2024-02\n",
      "ETL Procesado OK\n",
      "2024-03\n",
      "ETL Procesado OK\n",
      "2024-04\n",
      "ETL Procesado OK\n",
      "2024-05\n",
      "ETL Procesado OK\n",
      "2024-06\n",
      "ETL Procesado OK\n",
      "2024-07\n",
      "ETL Procesado OK\n",
      "2024-08\n",
      "ETL Procesado OK\n"
     ]
    }
   ],
   "source": [
    "# Función para leer la última fecha registrada y calcular el siguiente mes\n",
    "def get_start_date_from_csv(file_path):\n",
    "    try:\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            last_row = list(reader)[-1]  # Obtener la última fila\n",
    "            last_date = datetime.strptime(last_row[0], \"%d/%m/%Y\")  # Parsear la fecha en formato \"DD/MM/YYYY\"\n",
    "            return last_date + relativedelta(months=1)  # Retornar el mes siguiente\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error leyendo {file_path}: {e}\")\n",
    "        return datetime(2021, 1, 1)  # Valor predeterminado si no existe el archivo o falla la lectura\n",
    "\n",
    "\n",
    "# Función para verificar y descargar archivos .parquet\n",
    "def download_file(base_urls, output_folder, start_date, csv_url, fechas_csv, retries=3, delay=5):\n",
    "    \n",
    "    # Para cada fecha desde el inicio hasta el mes actual:\n",
    "    end_date = datetime.now()\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year_month = current_date.strftime(\"%Y-%m\")  # Formato año-mes\n",
    "        all_files_downloaded = True  # Bandera para verificar descarga de todos los archivos .parquet\n",
    "\n",
    "        # Se recorren todas las URL de los archivos parquet.\n",
    "        for dataset_type, base_url in base_urls.items():\n",
    "            \n",
    "            # Construir la URL y el nombre de archivo de destino\n",
    "            url = base_url.format(year_month)\n",
    "            file_name = f\"{dataset_type}_tripdata_{year_month}.parquet\"\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            # Intentar descargar el archivo si no existe\n",
    "            if not os.path.exists(file_path):\n",
    "                success = download_single_file(url, file_path, retries, delay)\n",
    "                if not success:\n",
    "                    all_files_downloaded = False  # Marcar como falso si falla alguna descarga\n",
    "\n",
    "        \n",
    "        # Si logró descargar los 4 archivos parquet del mes:\n",
    "        if all_files_downloaded:\n",
    "\n",
    "            # Descarga el archivo CSV mensual           \n",
    "            download_single_file(csv_url, csv_file_path, retries, delay)\n",
    "            \n",
    "            # Realiza el ETL de los 4 archivos parquet\n",
    "            ETL_Viajes_Diarios (output_folder,year_month)\n",
    "\n",
    "            # Guardar la fecha en Fechas_Archivos_Levantados.csv\n",
    "            save_date_to_csv(current_date, fechas_csv)\n",
    "\n",
    "        # Avanzar al siguiente mes\n",
    "        current_date += relativedelta(months=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para descargar un archivo individual con reintentos\n",
    "def download_single_file(url, file_path, retries=3, delay=5):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "            return True  # Éxito en la descarga\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                error_type = type(e).__name__\n",
    "                logging.error(f\"Error {error_type} en {file_path}: {e}\")\n",
    "            else:\n",
    "                time.sleep(delay)  # Esperar antes de intentar de nuevo\n",
    "    return False  # Falla después de todos los intentos\n",
    "\n",
    "\n",
    "\n",
    "# Función para guardar la fecha de descarga en el archivo CSV\n",
    "def save_date_to_csv(date, file_path):\n",
    "    try:\n",
    "        with open(file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([date.strftime(\"%d/%m/%Y\")])  # Guardar la fecha en formato \"DD/MM/YYYY\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error guardando fecha en {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################################\n",
    "################ MAIN ################\n",
    "################################################################################################################################################\n",
    "\n",
    "output_folder = r\"C:\\\\Users\\\\Hernán\\\\Documents\\\\datasets\\\\TLC Trip Record Data\"  # Ruta del Bucket\n",
    "\n",
    "\n",
    "# Configuración de logging para registrar errores en un archivo log\n",
    "log_file = output_folder + \"\\\\error_log.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuración de ruta y URLs base\n",
    "base_urls = {\n",
    "    \"yellow\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{}.parquet\",\n",
    "    \"green\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{}.parquet\",\n",
    "    \"fhv\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_{}.parquet\",\n",
    "    \"fhvhv\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_{}.parquet\",\n",
    "}\n",
    "\n",
    "csv_url = \"https://www.nyc.gov/assets/tlc/downloads/csv/data_reports_monthly.csv\"\n",
    "csv_file_path = output_folder + \"\\\\data_reports_monthly.csv\"\n",
    "\n",
    "\n",
    "# Llamada a la función principal\n",
    "fechas_csv = output_folder + \"\\\\Fechas_Archivos_Levantados.csv\"\n",
    "start_date = get_start_date_from_csv(fechas_csv)\n",
    "download_file(base_urls, output_folder, start_date, csv_url, fechas_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
